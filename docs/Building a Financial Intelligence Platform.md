

# **Architecting Ashworth Engine v2: An Incremental Build Plan for a Full-Stack, Containerized AI Platform**

## **Introduction**

This document provides an exhaustive architectural blueprint for the development of Ashworth Engine v2, a full-stack, containerized financial intelligence platform. The plan is specifically engineered for a solo developer, emphasizing an incremental build-and-test methodology. This approach deconstructs a complex system into a series of manageable, verifiable phases, ensuring that each component is stable and functional before the next is integrated. The architecture represents a modern paradigm for AI application development, strategically leveraging a containerized monorepo to maximize developer velocity and maintain a clean, scalable codebase.

The core of the platform is a sophisticated multi-agent system built with Python and LangGraph, capable of complex reasoning and task delegation. This system is augmented by a Retrieval-Augmented Generation (RAG) component, which uses a Supabase-hosted PostgreSQL database with the pgvector extension as its dynamic knowledge base. The user interface is a responsive, modern web application constructed with Next.js, accelerated by Turbopack, and styled with the highly customizable shadcn/ui component library.

The entire ecosystem is unified within a polyglot monorepo managed by Turborepo for high-level task orchestration and uv for high-performance Python environment management. This dual-tooling strategy provides a robust foundation for mixed-language development. Finally, the entire stack is containerized using Docker Compose, creating a reproducible, isolated, and production-ready development environment. This report will detail the step-by-step construction of this platform, from the initial workspace scaffolding to the final, feature-complete application.

## **Phase 1: Establishing the Monorepo Foundation**

The initial phase is dedicated to constructing the project's skeleton: a hybrid monorepo that elegantly accommodates both JavaScript/TypeScript and Python ecosystems. A well-architected monorepo is paramount for a solo developer, as it simplifies dependency management, streamlines the development workflow, and prevents the project's complexity from becoming unmanageable as it scales.1

### **1.1 Workspace Architecture and Initial Scaffolding**

The foundation will be bootstrapped using Turborepo's command-line interface, which provides a robust starting point for monorepo projects.2 The command

pnpm dlx create-turbo@latest will be used to generate an initial structure, which will then be customized to support the specific needs of a mixed-language environment.

The canonical directory structure is designed for a clear separation of concerns:

* **apps/**: This directory houses the deployable applications.  
  * web/: The Next.js frontend application, created using create-next-app.3  
  * api/: The Python backend, which will contain the FastAPI application and the LangGraph agent system.  
* **packages/**: This directory contains code and configurations shared across the monorepo.  
  * ui/: A shared library of React components built with shadcn/ui, consumable by the web application.4  
  * eslint-config-custom/ & tsconfig/: Centralized configurations for ESLint and TypeScript to enforce consistent coding standards across all JavaScript/TypeScript packages.4  
  * py-common/: A shared Python library for common utilities, data models (e.g., Pydantic schemas), or constants used by the api application.  
* **infra/**: This directory isolates all infrastructure-as-code.  
  * docker-compose.yml: The central orchestration file for all containerized services.  
  * supabase/: Contains the configuration files generated by the Supabase CLI for the local development environment.

This structure ensures that application code (apps), shared logic (packages), and infrastructure definitions (infra) are logically separated, making the repository easier to navigate and maintain.

### **1.2 Configuring Turborepo for a Polyglot Environment**

Turborepo serves as the high-performance build system and task orchestrator for the monorepo.5 Its primary configuration file,

turbo.json, will be defined at the root of the repository to manage the build pipeline and development tasks across both the Next.js frontend and the Python backend.

A sample turbo.json configuration is as follows:

JSON

{  
  "$schema": "https://turborepo.org/schema.json",  
  "pipeline": {  
    "build": {  
      "dependsOn": \["^build"\],  
      "outputs": \[".next/\*\*", "\!.next/cache/\*\*", "dist/\*\*"\]  
    },  
    "lint": {},  
    "dev": {  
      "cache": false,  
      "persistent": true  
    },  
    "clean": {  
      "cache": false  
    }  
  }  
}

This configuration defines several key tasks:

* build: Specifies that building a package depends on the successful build of its internal dependencies (^build). It also defines the output directories to be cached, which is crucial for Turborepo's performance gains.5  
* dev: The primary task for local development. Caching is disabled, and the task is marked as persistent because it runs long-lived processes like development servers.  
* lint and clean: Standard tasks for code quality checks and cleaning build artifacts.

Scripts in the root package.json will act as the main entry points, delegating execution to Turborepo. For instance, a root dev script will use Turborepo to concurrently launch the Next.js and FastAPI development servers.

### **1.3 Python Workspace Integration with uv**

To properly manage the Python side of the monorepo, a more specialized tool than Turborepo's generic task runner is required. uv, a high-performance Python package manager and resolver written in Rust, provides robust support for monorepo workspaces, inspired by Rust's Cargo workspaces.7 This creates a two-tiered management system. Turborepo acts as the high-level "general contractor," orchestrating cross-language tasks like

dev and build. uv, in turn, acts as the "specialist subcontractor," handling the intricacies of Python dependency resolution, virtual environment management, and inter-package linking. This layered approach is superior to relying on fragile shell scripts within package.json, as it leverages a tool purpose-built for Python monorepos, ensuring consistency and reliability.9

First, a pyproject.toml file is created at the monorepo root to define the uv workspace. This file makes uv aware of all Python packages within the repository 9:

Ini, TOML

\[tool.uv.workspace\]  
members \= \["apps/api", "packages/py-common"\]

Next, the pyproject.toml file within the apps/api directory is configured to declare a dependency on the local py-common package. The tool.uv.sources table instructs uv to resolve this dependency from within the workspace rather than from an external package index like PyPI.9

Ini, TOML

\# In apps/api/pyproject.toml  
\[project\]  
name \= "api"  
dependencies \= \[  
    "py-common"  
\]

\[tool.uv.sources\]  
py-common \= { workspace \= true }

This configuration ensures that any changes made in the py-common library are immediately available to the api application during development, as uv treats workspace dependencies as editable installs.9 This setup creates a unified

uv.lock file at the workspace root, guaranteeing a single, consistent set of resolved dependencies for all Python packages, which is critical for avoiding dependency conflicts.10

#### **Checkpoint 1**

At the conclusion of this phase, the monorepo foundation is solid. The developer can execute pnpm install from the root to install all JavaScript dependencies and create the unified Python virtual environment managed by uv. Running pnpm dev will successfully launch the Next.js frontend development server and a placeholder FastAPI backend server simultaneously, orchestrated by Turborepo. The Python environment is fully self-contained and reproducible.

### **1.4 Monorepo Directory and Configuration Overview**

To provide a clear map of the project architecture, the following table summarizes the purpose of each key directory and configuration file established in this phase. This serves as essential internal documentation, reducing the cognitive load for navigating a complex, polyglot codebase.

| Path | Purpose | Key Technologies |
| :---- | :---- | :---- |
| apps/web/ | Next.js frontend application | Next.js, Turbopack, React |
| apps/api/ | Python backend application | FastAPI, LangGraph, uv |
| packages/ui/ | Shared shadcn/ui components | React, Tailwind CSS |
| packages/py-common/ | Shared Python code (e.g., data models) | Python, uv |
| infra/ | Docker and Supabase configurations | Docker Compose |
| turbo.json | Monorepo task orchestration | Turborepo |
| pyproject.toml (root) | Defines the Python workspace | uv |

## **Phase 2: Containerizing the Full Stack with Docker and Supabase**

This phase transitions the project from a locally-run setup to a fully containerized environment using Docker Compose. This is a critical step that ensures development parity with production, eliminates environment-specific issues, and simplifies the setup process into a single command.

### **2.1 Local Supabase Setup with the CLI**

The foundation of our data and authentication layer will be a local instance of Supabase, managed via the Supabase CLI. This provides a complete, self-contained Supabase stack, including Postgres, GoTrue for authentication, and the Kong API gateway, all running in Docker containers.11

Within the infra/ directory, the following commands are executed:

1. supabase init: This command creates a new supabase directory containing the necessary configuration files for a local Supabase project.12  
2. supabase start: This command downloads the required Docker images and starts all the Supabase services. The first run may take some time.11

Upon successful startup, the local Supabase Studio will be accessible at http://localhost:54323, providing a graphical interface to manage the local database, authentication, and other services.12

### **2.2 Orchestration with a Unified docker-compose.yml**

To manage our custom applications alongside the Supabase stack, a root docker-compose.yml file is created in the infra/ directory. This file will orchestrate the web (Next.js) and api (FastAPI) services and integrate the Supabase services.

A key best practice is to use Docker Compose's include feature. Instead of copying and pasting the contents of the supabase/docker-compose.yml file, we include it directly. This keeps the Supabase configuration modular, making it easy to update in the future by simply pulling the latest changes into the supabase directory.13

The docker-compose.yml will also define our custom services. A crucial aspect of this configuration is networking. A common pitfall in multi-container setups is inter-service communication; for example, a request from the Next.js container to localhost:8000 will fail because localhost within a container refers to the container itself, not the host machine or other containers.13 To solve this, a custom Docker network is defined, and all services (

web, api, and the Supabase services) are attached to it. This allows services to resolve each other by their service name (e.g., api, kong).

A simplified docker-compose.yml would look like this:

YAML

version: '3.8'

services:  
  web:  
    build:  
      context:../  
      dockerfile:./apps/web/Dockerfile  
    ports:  
      \- "3000:3000"  
    env\_file:  
      \-.env  
    networks:  
      \- ashworth-net

  api:  
    build:  
      context:../  
      dockerfile:./apps/api/Dockerfile  
    ports:  
      \- "8001:8000" \# Expose API on 8001 to avoid conflict with Supabase's Kong  
    env\_file:  
      \-.env  
    networks:  
      \- ashworth-net

\# Include Supabase services and attach them to our network  
include:  
  \- path:./supabase/docker-compose.yml  
    env\_file:.env

networks:  
  ashworth-net:  
    driver: bridge

### **2.3 Unified Environment Management**

Consistent environment configuration is vital. A single .env file at the root of the infra/ directory will serve as the single source of truth for all configuration variables, such as database credentials and API keys. The docker-compose.yml file uses the env\_file directive to inject these variables into the appropriate containers, ensuring that all services share a consistent configuration.

#### **Checkpoint 2**

With the completion of this phase, the entire platform is containerized. A single command, docker compose up \--build, executed from the infra/ directory, will build the application images and launch the entire stack: the Next.js frontend, the FastAPI backend, and all local Supabase services. The services will be healthy and capable of communicating with each other over the custom Docker network. The developer can access the web application at http://localhost:3000 and the local Supabase Studio at http://localhost:54323.

### **2.4 Service Environment Variables**

The following table details the essential environment variables required for the system to function correctly. It clarifies the purpose of each variable and, critically, distinguishes between URLs used for external access (from the browser) and those used for internal communication between containers on the Docker network. This explicit distinction preemptively solves a common class of networking and configuration errors.13

| Variable | Service(s) | Example Value | Purpose |
| :---- | :---- | :---- | :---- |
| POSTGRES\_PASSWORD | Supabase | your-super-secret-password | Sets the local Postgres superuser password.14 |
| SUPABASE\_ANON\_KEY | Supabase, Web | ey... | Public-facing key for client-side Supabase access.13 |
| SUPABASE\_SERVICE\_ROLE\_KEY | Supabase, API | ey... | Secret key for backend services to bypass RLS. |
| DATABASE\_URL | API | postgresql://postgres:...@db:5432/postgres | Direct connection string for the API to the Postgres container over the Docker network. |
| NEXT\_PUBLIC\_SUPABASE\_URL | Web | http://localhost:8000 | URL for the browser to reach the local Supabase API gateway (Kong).13 |
| NEXT\_PUBLIC\_API\_URL | Web | http://localhost:8001 | URL for the browser to reach the local FastAPI backend. |
| INTERNAL\_SUPABASE\_URL | API | http://kong:8000 | URL for the API container to reach the Supabase gateway over the Docker network.13 |

## **Phase 3: Building the RAG Knowledge Ingestion Pipeline**

With a robust, containerized infrastructure in place, the focus now shifts to implementing the first core feature: a pipeline to ingest financial documents into the knowledge base. This process involves setting up the vector database, creating a backend service to process documents, and storing them as embeddings for future retrieval. Building this ingestion pipeline as a discrete, testable component before developing the query-side agent system is a deliberate architectural choice. It allows the developer to perfect the complex logic of document loading, chunking, and embedding in isolation, simplifying debugging and ensuring the quality of the data in the knowledge base.

### **3.1 Configuring the Supabase Vector Store**

The Supabase local development workflow includes a powerful migration system that allows for version-controlled schema changes.15 This system will be used to prepare the PostgreSQL database for storing vector embeddings.

1. **Migration 1: Enable pgvector**: The first step is to enable the pgvector extension, which adds the vector data type and similarity search functions to PostgreSQL.16 A new migration file is created (  
   supabase migration new enable\_pgvector) and populated with a single SQL command: create extension if not exists vector;.  
2. **Migration 2: Create Knowledge Base Table**: The next migration (supabase migration new create\_documents\_table) will define the schema for storing our processed knowledge. This table will hold the text chunks, associated metadata, and the vector embeddings.17 The dimensionality of the  
   embedding column is critical and must precisely match the output dimensions of the chosen embedding model (e.g., 1536 for OpenAI's text-embedding-ada-002, or 384 for Supabase/gte-small 16).  
   SQL  
   create table documents (  
     id bigserial primary key,  
     content text,  
     metadata jsonb,  
     embedding vector(1536)  
   );

3. **Migration 3: Create Search Function**: The LangChain SupabaseVectorStore integration relies on a specific PostgreSQL function to perform similarity searches efficiently. This function, typically named match\_documents, encapsulates the logic for calculating cosine similarity between a query vector and the stored embeddings.17 A third migration (  
   supabase migration new create\_match\_documents\_function) will be created to add this PL/pgSQL function to the database.

These migrations are applied to the local database by running supabase db reset, which tears down and recreates the database, applying all migration files in chronological order.15

### **3.2 Developing the FastAPI Ingestion Service**

A new endpoint, POST /upload, will be added to the api service. This endpoint will orchestrate the entire ingestion workflow.

1. **File Handling**: The endpoint will be defined to accept file uploads using FastAPI's UploadFile type, which provides an asynchronous interface for handling file data.18 This requires the  
   python-multipart library to be installed.20  
2. **Document Loading**: Upon receiving a file, the service will use LangChain's extensive library of DocumentLoader integrations to parse the content.21 The implementation can start with a  
   PyPDFLoader for PDF files and be designed to be extensible, perhaps by detecting the file's MIME type and selecting the appropriate loader dynamically.23  
3. **Text Splitting**: Raw documents are often too large to be effectively embedded or processed by language models.24 The loaded text will be segmented into smaller, semantically coherent chunks using a text splitter like  
   RecursiveCharacterTextSplitter.25 This splitter intelligently attempts to break the text along natural boundaries like paragraphs and sentences before resorting to character-level splits. The  
   chunk\_size and chunk\_overlap parameters are critical hyperparameters that directly impact the quality of the subsequent retrieval process and must be tuned carefully.26  
4. **Embedding and Storage**: The final step is to convert the text chunks into vector embeddings and store them in the Supabase database. The LangChain SupabaseVectorStore provides a high-level from\_documents() method that streamlines this process.17 This single method handles:  
   * Initializing a connection to the Supabase instance using the provided client.  
   * Invoking an embedding model (e.g., OpenAIEmbeddings) for each text chunk to generate a vector.  
   * Batch-inserting the original text, metadata, and the newly generated embedding into the documents table.

#### **Checkpoint 3**

The ingestion pipeline is now complete and testable. Using an API client like Insomnia or Postman, the developer can send a POST request with a file (e.g., a PDF) to the http://localhost:8001/upload endpoint. After the request completes successfully, they can connect to the local PostgreSQL database (via the Supabase Studio or another client) and query the documents table to verify that it has been populated with multiple rows, each containing a text chunk and a vector embedding.

## **Phase 4: Implementing the LangGraph Multi-Agent System**

This phase involves constructing the core intelligence of the platform: a multi-agent system powered by LangGraph. LangGraph allows for the creation of cyclical, stateful graphs, enabling more complex and robust agentic workflows than linear chains.27 This system will be designed to reason, delegate tasks between specialized agents, and utilize the knowledge base created in the previous phase.

### **4.1 Designing the Agentic Graph with LangGraph**

The agent system will be modeled as a stateful graph. The state object will be passed between nodes and will persist information throughout the execution, such as the initial user query, the conversational history, retrieved documents, and intermediate agent outputs.

The graph will consist of several key nodes and edges:

* **Nodes (Agents and Tools)**: Each node represents a unit of computation.  
  * Router: This is a critical conditional node. After each step, the router will examine the current state and decide the next action. This could be routing to a specific tool, asking the user for clarification, or generating the final response.  
  * Retriever\_Agent: A specialized agent whose sole purpose is to interact with the knowledge base. It will be equipped with a tool to query the Supabase vector store.  
  * Financial\_Analyst\_Agent: This agent is responsible for higher-level reasoning. It takes the raw information retrieved by the Retriever\_Agent and synthesizes it into a coherent, analytical answer, potentially performing calculations or interpreting financial data.  
* **Edges**: These define the flow of control and data between nodes. The graph will feature conditional edges originating from the Router, allowing for dynamic, non-linear execution paths based on the evolving state of the conversation.

### **4.2 Tooling the Retriever Agent**

The Retriever\_Agent's effectiveness depends on its ability to query the RAG knowledge base. This is achieved by creating a custom tool using LangChain's abstractions.

The tool will be a Python function that encapsulates the logic for performing a similarity search. Internally, this function will instantiate the SupabaseVectorStore, connecting it to the local Supabase instance using the credentials from the environment variables. The key method is .as\_retriever(), which converts the vector store into a standard LangChain Retriever object.17 This retriever interface is what the agent will invoke. When the agent decides to use this tool, it passes the user's query to the retriever, which in turn executes the

match\_documents function in PostgreSQL to find the most relevant document chunks.

### **4.3 Exposing the Graph via a Streaming FastAPI Endpoint**

To create an interactive and responsive user experience, the LangGraph agent will be exposed via a new streaming API endpoint, POST /chat, in the FastAPI application.

* **Streaming Responses**: Traditional request-response cycles are ill-suited for LLM interactions, which can have high latency. A streaming response provides a much better user experience by sending back parts of the response as they are generated. FastAPI's StreamingResponse is ideal for this. The backend will invoke the LangGraph agent using its .stream() method, which yields intermediate outputs from each node in the graph as they execute. Each yielded chunk of data can be formatted (e.g., as a Server-Sent Event) and sent immediately to the client.27  
* **Session Management**: To support multi-turn conversations, the endpoint must manage conversational history. A simple session management system can be implemented, perhaps using a unique session ID provided by the client. This allows the backend to retrieve the history for a given conversation and include it in the agent's state, providing crucial context for follow-up questions.27

#### **Checkpoint 4**

The backend "engine" is now fully operational. The developer can use an API client to send a JSON payload containing a query to the http://localhost:8001/chat endpoint. This action will trigger the LangGraph execution flow. By observing the logs or the streamed response, one can trace the execution path: the router directing the query to the retriever, the retriever tool fetching relevant document chunks from the Supabase vector store, and the financial analyst agent synthesizing a final answer, all streamed back in near real-time.

## **Phase 5: Constructing the Interactive Frontend**

The final phase focuses on building the user interface that allows a user to interact with the powerful backend systems developed in the preceding phases. The frontend will be a clean, responsive, and intuitive single-page application built with Next.js.

### **5.1 UI Foundation with shadcn/ui**

shadcn/ui is not a traditional component library but a collection of reusable components that are copied directly into the project's codebase, giving the developer full control over their code, styling, and behavior.29

The first step is to initialize shadcn/ui within the apps/web Next.js project using its CLI: pnpm dlx shadcn@latest init. This command will prompt for configuration options (such as style and color theme) and set up the necessary files, including tailwind.config.js and components.json.30

Following initialization, individual components required for the application will be added using the CLI. For Ashworth Engine v2, a core set of components would include button, input, card, label, and potentially more complex ones like data-table for displaying structured data.32 For example, to add the input component, the command would be

pnpm dlx shadcn@latest add input.31

### **5.2 Building the Document Upload Interface**

A dedicated page or section of the UI will be created for uploading documents to the knowledge base. This will be implemented as a React component.

This component will utilize the shadcn/ui Input and Label components to create a visually appealing and accessible file selection interface. The Input component will be configured with type="file".31 React's

useState hook will be used to manage the state of the selected file. An onSubmit handler will be implemented for the form, which will use the browser's Fetch API or a library like axios to construct a multipart/form-data request and send the selected file to the backend's POST /upload endpoint (created in Phase 3).

### **5.3 Building the Conversational Chat Interface**

This is the central feature of the application. A new component will be built to serve as the chat interface. It will consist of two main parts: a display area for the conversation history and an input form for the user to submit new messages.

* **API Integration and State Management**: When a user submits a query through the input form, the component will trigger an asynchronous function. This function will make a POST request to the backend's /chat endpoint (from Phase 4), sending the current user message and potentially the conversation history for context.  
* **Handling Streaming Data**: The true interactivity of the chat interface comes from handling the streaming response from the backend. The Fetch API in modern browsers natively supports ReadableStream. The frontend code will await the response body and use a for await...of loop to read the data chunks as they arrive from the server. As each new chunk (representing a token or an intermediate step from the LangGraph agent) is received, the component's state will be updated. This state update will trigger a re-render, appending the new text to the AI's response on the screen, creating the characteristic token-by-token typing effect seen in modern AI chatbots.

#### **Checkpoint 5**

The Ashworth Engine v2 is now feature-complete for its initial version. The frontend application is fully integrated with the backend. A user can open the web application in their browser, navigate to the upload page to add a new financial document to the knowledge base, and then proceed to the chat interface. In the chat interface, they can ask questions about the content of the uploaded document, and the AI agent's response will be streamed to the screen in real-time, providing a seamless and interactive experience.

## **Conclusion and Future Trajectory**

This report has detailed an incremental, phase-by-phase plan for a solo developer to construct the Ashworth Engine v2, a sophisticated, full-stack financial intelligence platform. By leveraging a containerized monorepo architecture, the plan systematically addresses the complexities of integrating a diverse technology stack, including a Next.js frontend, a Python/LangGraph multi-agent backend, and a Supabase/pgvector RAG pipeline. The emphasis on discrete, testable checkpoints at the end of each phase de-risks the development process and ensures a stable, functional system at every stage of the build. The resulting architecture is robust, scalable, and maintainable—a testament to the power of modern development practices and tooling.

The completion of this plan yields a powerful version 1.0 of the platform. However, the architectural foundation laid here is designed for future growth. The following areas represent a logical roadmap for continued development:

* **Production Deployment**: The containerized nature of the application simplifies the transition to production. A viable strategy would involve deploying the Next.js frontend to a platform like Vercel, which is highly optimized for it.5 The FastAPI backend and the Supabase stack (or a managed Supabase project) could be deployed to a container orchestration service such as Fly.io, Render, or AWS Elastic Container Service (ECS).  
* **Observability and Monitoring**: For a production system, robust monitoring is non-negotiable. Integrating an LLM observability platform like Langfuse would provide invaluable insights into agent traces, performance, and costs, a practice recommended in production-ready templates.28  
* **Advanced RAG Strategies**: The current RAG implementation uses a basic similarity search. Retrieval quality could be significantly enhanced by implementing more advanced techniques. This could include hybrid search (combining vector search with traditional keyword search), which Supabase supports, or more complex strategies like parent-document retrievers, where smaller chunks are retrieved for accuracy, but the surrounding context from the larger parent document is returned to the LLM.  
* **Scalability and Performance**: As user load and data volume increase, performance bottlenecks may emerge. The pgvector extension supports approximate nearest neighbor (ANN) indexes like IVFFlat and HNSW, which are essential for maintaining fast query performance on large datasets.33 Proactive indexing of the  
  embedding column will be a critical optimization step. Furthermore, the backend API service can be scaled horizontally by running multiple container instances behind a load balancer.

#### **Works cited**

1. Mastering Next.js Monorepos: A Comprehensive Guide | by Omar Shiriniani | Medium, accessed August 26, 2025, [https://medium.com/@omar.shiriniani/mastering-next-js-monorepos-a-comprehensive-guide-15f59f5ef615](https://medium.com/@omar.shiriniani/mastering-next-js-monorepos-a-comprehensive-guide-15f59f5ef615)  
2. Start with an example \- Turborepo, accessed August 26, 2025, [https://turborepo.com/docs/getting-started/examples](https://turborepo.com/docs/getting-started/examples)  
3. Next.js \- Turborepo, accessed August 26, 2025, [https://turborepo.com/docs/guides/frameworks/nextjs](https://turborepo.com/docs/guides/frameworks/nextjs)  
4. godkingjay/turborepo-next-django-starter \- GitHub, accessed August 26, 2025, [https://github.com/godkingjay/turborepo-next-django-starter](https://github.com/godkingjay/turborepo-next-django-starter)  
5. Deploying Turborepo to Vercel, accessed August 26, 2025, [https://vercel.com/docs/monorepos/turborepo](https://vercel.com/docs/monorepos/turborepo)  
6. Turborepo, accessed August 26, 2025, [https://turborepo.com/](https://turborepo.com/)  
7. una: About, accessed August 26, 2025, [https://una.rdrn.me/](https://una.rdrn.me/)  
8. Releasing a Monorepo using uv Workspace and Python Semantic Release \- Medium, accessed August 26, 2025, [https://medium.com/@asafshakarzy/releasing-a-monorepo-using-uv-workspace-and-python-semantic-release-0dafc889f4cc](https://medium.com/@asafshakarzy/releasing-a-monorepo-using-uv-workspace-and-python-semantic-release-0dafc889f4cc)  
9. Using workspaces | uv \- Astral Docs, accessed August 26, 2025, [https://docs.astral.sh/uv/concepts/projects/workspaces/](https://docs.astral.sh/uv/concepts/projects/workspaces/)  
10. Cracking the Python Monorepo: build pipelines with uv and Dagger \- Reddit, accessed August 26, 2025, [https://www.reddit.com/r/Python/comments/1iy4h5k/cracking\_the\_python\_monorepo\_build\_pipelines\_with/](https://www.reddit.com/r/Python/comments/1iy4h5k/cracking_the_python_monorepo_build_pipelines_with/)  
11. How to set up Supabase local development environment? \- Bootstrapped, accessed August 26, 2025, [https://bootstrapped.app/guide/how-to-set-up-supabase-local-development-environment](https://bootstrapped.app/guide/how-to-set-up-supabase-local-development-environment)  
12. Local Development & CLI | Supabase Docs, accessed August 26, 2025, [https://supabase.com/docs/guides/local-development](https://supabase.com/docs/guides/local-development)  
13. Setting up frontend together with supabase-docker in a single ..., accessed August 26, 2025, [https://github.com/orgs/supabase/discussions/36296](https://github.com/orgs/supabase/discussions/36296)  
14. Self-Hosting with Docker | Supabase Docs, accessed August 26, 2025, [https://supabase.com/docs/guides/self-hosting/docker](https://supabase.com/docs/guides/self-hosting/docker)  
15. Local development with schema migrations | Supabase Docs, accessed August 26, 2025, [https://supabase.com/docs/guides/local-development/overview](https://supabase.com/docs/guides/local-development/overview)  
16. Vector columns | Supabase Docs, accessed August 26, 2025, [https://supabase.com/docs/guides/ai/vector-columns](https://supabase.com/docs/guides/ai/vector-columns)  
17. SupabaseVectorStore | 🦜️ Langchain, accessed August 26, 2025, [https://js.langchain.com/docs/integrations/vectorstores/supabase/](https://js.langchain.com/docs/integrations/vectorstores/supabase/)  
18. Request Files \- FastAPI, accessed August 26, 2025, [https://fastapi.tiangolo.com/tutorial/request-files/](https://fastapi.tiangolo.com/tutorial/request-files/)  
19. fastapi-upload-example/main.py at master \- GitHub, accessed August 26, 2025, [https://github.com/louis70109/fastapi-example/blob/master/main.py](https://github.com/louis70109/fastapi-example/blob/master/main.py)  
20. Request Forms and Files \- FastAPI, accessed August 26, 2025, [https://fastapi.tiangolo.com/tutorial/request-forms-and-files/](https://fastapi.tiangolo.com/tutorial/request-forms-and-files/)  
21. Document loaders \- Python LangChain, accessed August 26, 2025, [https://python.langchain.com/docs/concepts/document\_loaders/](https://python.langchain.com/docs/concepts/document_loaders/)  
22. document\_loaders — LangChain documentation, accessed August 26, 2025, [https://python.langchain.com/api\_reference/community/document\_loaders.html](https://python.langchain.com/api_reference/community/document_loaders.html)  
23. Custom Document Loader \- Python LangChain, accessed August 26, 2025, [https://python.langchain.com/docs/how\_to/document\_loader\_custom/](https://python.langchain.com/docs/how_to/document_loader_custom/)  
24. Text Splitters in LangChain for Data Processing | by Mangesh Salunke \- Medium, accessed August 26, 2025, [https://medium.com/data-and-beyond/text-splitters-in-langchain-for-data-processing-3a958eea2797](https://medium.com/data-and-beyond/text-splitters-in-langchain-for-data-processing-3a958eea2797)  
25. Text splitters \- Python LangChain, accessed August 26, 2025, [https://python.langchain.com/docs/concepts/text\_splitters/](https://python.langchain.com/docs/concepts/text_splitters/)  
26. TextSplitter — LangChain documentation, accessed August 26, 2025, [https://python.langchain.com/api\_reference/text\_splitters/base/langchain\_text\_splitters.base.TextSplitter.html](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html)  
27. Integrating LangGraph RAG Agent with FastAPI | Production Setup with Sessions, History, Vector DB \- YouTube, accessed August 26, 2025, [https://www.youtube.com/watch?v=t209A887UpY](https://www.youtube.com/watch?v=t209A887UpY)  
28. I've made a production-ready Fastapi LangGraph template : r/LangChain \- Reddit, accessed August 26, 2025, [https://www.reddit.com/r/LangChain/comments/1juejy2/ive\_made\_a\_productionready\_fastapi\_langgraph/](https://www.reddit.com/r/LangChain/comments/1juejy2/ive_made_a_productionready_fastapi_langgraph/)  
29. Introduction \- Shadcn UI, accessed August 26, 2025, [https://ui.shadcn.com/docs](https://ui.shadcn.com/docs)  
30. Introduction to Shadcn/ui Library \- DEV Community, accessed August 26, 2025, [https://dev.to/shreyvijayvargiya/introduction-to-shadcnui-library-g79](https://dev.to/shreyvijayvargiya/introduction-to-shadcnui-library-g79)  
31. Input \- shadcn/ui, accessed August 26, 2025, [https://ui.shadcn.com/docs/components/input](https://ui.shadcn.com/docs/components/input)  
32. Shadcn UI for Beginners: The Ultimate Step-by-Step Tutorial \- CodeParrot AI, accessed August 26, 2025, [https://codeparrot.ai/blogs/shadcn-ui-for-beginners-the-ultimate-guide-and-step-by-step-tutorial](https://codeparrot.ai/blogs/shadcn-ui-for-beginners-the-ultimate-guide-and-step-by-step-tutorial)  
33. API | Supabase Docs, accessed August 26, 2025, [https://supabase.com/docs/guides/ai/python/api](https://supabase.com/docs/guides/ai/python/api)